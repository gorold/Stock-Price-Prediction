{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping EDGAR website for 8-K documents\n",
    "The SEC EDGAR website uses the CIK codes in the URLs for all SEC registered company.\n",
    "\n",
    "Using the CIK codes, we can scrape the SEC website for all links of 8-K documents of the copmanies that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tickers of the companies that we want to scrape\n",
    "tickers = {'AAPL', 'MSFT', 'AMZN', 'BRK', 'FB', 'JPM', 'JNJ', 'XOM', 'GOOG', 'BAC'}\n",
    "cik = {}\n",
    "\n",
    "# Obtain the CIK for each company in ticker\n",
    "# http://rankandfiled.com/#/data/tickers\n",
    "with open('cik_ticker.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        list_form = line.strip().split('|')\n",
    "        if list_form[1] in tickers:\n",
    "            cik[list_form[1]] = list_form[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EDGAR website uses a search function, but can only list a maximum of 100 8-K document listings per page. Thus, we have to get the links for all serach pages of the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT: 3\n",
      "XOM: 3\n"
     ]
    }
   ],
   "source": [
    "company_links = {k:[] for k in tickers}\n",
    "for ticker, cik_num in cik.items():\n",
    "    page_num = 0\n",
    "    while True:\n",
    "        url = (\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={0}\"\n",
    "        \"&type=8-K%25&dateb=&owner=exclude&start={1}&count=100\").format(cik_num, page_num*100)\n",
    "        test_html = urlopen(url)\n",
    "        soup = BeautifulSoup(test_html, 'html.parser')\n",
    "        urls_return = soup.find_all(id='documentsbutton')\n",
    "        if len(urls_return) == 0:\n",
    "            print(ticker, \": \", page_num, sep='')\n",
    "            break\n",
    "        else:\n",
    "            company_links[ticker].append(url)\n",
    "            page_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the 8-K page links for each search page result\n",
    "def get_8K_links(input_link):\n",
    "    input_html = urlopen(input_link)\n",
    "    soup = BeautifulSoup(input_html, 'html.parser')\n",
    "    urls_return = soup.find_all(id='documentsbutton')\n",
    "    return [link.get('href') for link in urls_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_8K_links = {k:[] for k in tickers}\n",
    "for ticker in company_links:\n",
    "    for search_link in company_links[ticker]:\n",
    "        out = get_8K_links(search_link)\n",
    "        company_8K_links[ticker] += out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir('Data')\n",
    "except FileNotFoundError:\n",
    "    os.mkdir('Data')\n",
    "    os.chdir('Data')\n",
    "for ticker in company_8K_links:\n",
    "    try:\n",
    "        os.chdir(ticker)\n",
    "    except FileNotFoundError:\n",
    "        os.mkdir(ticker)\n",
    "        os.chdir(ticker)\n",
    "    \n",
    "    with open('8K_links.txt', 'w') as file:\n",
    "        for line in company_8K_links[ticker]:\n",
    "            file.write(line + '\\n')\n",
    "    \n",
    "    os.chdir('..')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html = urlopen('https://www.sec.gov/Archives/edgar/data/320193/000118143112038301/0001181431-12-038301-index.htm')\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for thing in soup.find_all('div', attrs={'class': 'infoHead'}):\n",
    "    print(thing.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for thing in soup.find_all('div', attrs={'class': 'info'}):\n",
    "    print(thing.contents)\n",
    "    \n",
    "soup.find_all('div', attrs={'class': 'info'})[0].contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soup.find('table', attrs={'class': 'tableFile', 'summary':'Document Format Files'}).next.next_sibling.next_sibling.next_sibling.next.next.next.next.next.next.next.next.a.get('href')\n",
    "soup.find('table', attrs={'class': 'tableFile', 'summary':'Document Format Files'}).next.next_sibling.next_sibling.next_sibling.next.next.next.next.next.next.next.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup.find('table', attrs={'class': 'tableFile', 'summary':'Document Format Files'}).next.next_sibling.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup.find('table', attrs={'class': 'tableFile', 'summary':'Document Format Files'}).next.next_sibling.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\n",
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\\Data\n",
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\\Data\\MSFT\n",
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\\Data\n",
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\\Data\\XOM\n",
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\\Data\n",
      "C:\\Users\\wooge\\OneDrive\\Desktop\\SEC Data\n"
     ]
    }
   ],
   "source": [
    "prefix = 'https://www.sec.gov/'\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('Data')\n",
    "print(os.getcwd())\n",
    "\n",
    "companies = os.listdir()\n",
    "companies = {'MSFT', 'XOM'}\n",
    "for company in companies:\n",
    "    os.chdir(company)\n",
    "    print(os.getcwd())\n",
    "    with open('8K_links.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            \n",
    "            url = urlopen(prefix+line)\n",
    "            soup = BeautifulSoup(url, 'html.parser')\n",
    "            \n",
    "            # Each 8-K entry has its own folder\n",
    "            folder_name = soup.find_all('div', attrs={'class': 'info'})[0].contents[0]\n",
    "            try:\n",
    "                os.chdir(folder_name)\n",
    "            except FileNotFoundError:\n",
    "                os.mkdir(folder_name)\n",
    "                os.chdir(folder_name)\n",
    "            \n",
    "            # Create details file\n",
    "            with open('details.txt', 'w') as file2:\n",
    "                for thing in soup.find_all('div', attrs={'class': 'info'}):\n",
    "                    file2.write(thing.contents[0] + '\\n')\n",
    "            \n",
    "            try:\n",
    "                download_this = soup.find('table', attrs={'class': 'tableFile', 'summary':'Document Format Files'}).next.next_sibling.next_sibling.next_sibling.next.next.next.next.next.next.next.next.a.get('href')\n",
    "            except AttributeError:\n",
    "                download_this = soup.find('table', attrs={'class': 'tableFile', 'summary':'Document Format Files'}).next.next_sibling.next_sibling.next_sibling.next.next.next.next.next.next.next.a.get('href')\n",
    "                \n",
    "            if '8K.html' in os.listdir():\n",
    "                os.chdir('..')\n",
    "            else:\n",
    "                urlretrieve(prefix+download_this, '8K.html')\n",
    "                os.chdir('..')\n",
    "            \n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())\n",
    "\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
