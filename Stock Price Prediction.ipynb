{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2218, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>216.820007</td>\n",
       "      <td>221.259995</td>\n",
       "      <td>216.630005</td>\n",
       "      <td>220.789993</td>\n",
       "      <td>220.789993</td>\n",
       "      <td>27693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>219.750000</td>\n",
       "      <td>222.820007</td>\n",
       "      <td>219.699997</td>\n",
       "      <td>222.190002</td>\n",
       "      <td>222.190002</td>\n",
       "      <td>24554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>223.750000</td>\n",
       "      <td>219.759995</td>\n",
       "      <td>220.419998</td>\n",
       "      <td>220.419998</td>\n",
       "      <td>23984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>2018-09-27</td>\n",
       "      <td>223.820007</td>\n",
       "      <td>226.440002</td>\n",
       "      <td>223.539993</td>\n",
       "      <td>224.949997</td>\n",
       "      <td>224.949997</td>\n",
       "      <td>30181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>224.789993</td>\n",
       "      <td>225.839996</td>\n",
       "      <td>224.020004</td>\n",
       "      <td>225.740005</td>\n",
       "      <td>225.740005</td>\n",
       "      <td>22929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>227.949997</td>\n",
       "      <td>229.419998</td>\n",
       "      <td>226.350006</td>\n",
       "      <td>227.259995</td>\n",
       "      <td>227.259995</td>\n",
       "      <td>23600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>227.250000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>226.630005</td>\n",
       "      <td>229.279999</td>\n",
       "      <td>229.279999</td>\n",
       "      <td>24788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>230.050003</td>\n",
       "      <td>233.470001</td>\n",
       "      <td>229.779999</td>\n",
       "      <td>232.070007</td>\n",
       "      <td>232.070007</td>\n",
       "      <td>28654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>230.779999</td>\n",
       "      <td>232.350006</td>\n",
       "      <td>226.729996</td>\n",
       "      <td>227.990005</td>\n",
       "      <td>227.990005</td>\n",
       "      <td>32042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>227.960007</td>\n",
       "      <td>228.410004</td>\n",
       "      <td>220.580002</td>\n",
       "      <td>224.289993</td>\n",
       "      <td>224.289993</td>\n",
       "      <td>33580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>222.210007</td>\n",
       "      <td>224.800003</td>\n",
       "      <td>220.199997</td>\n",
       "      <td>223.770004</td>\n",
       "      <td>223.770004</td>\n",
       "      <td>29663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>223.639999</td>\n",
       "      <td>227.270004</td>\n",
       "      <td>222.250000</td>\n",
       "      <td>226.869995</td>\n",
       "      <td>226.869995</td>\n",
       "      <td>26891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>225.460007</td>\n",
       "      <td>226.350006</td>\n",
       "      <td>216.050003</td>\n",
       "      <td>216.360001</td>\n",
       "      <td>216.360001</td>\n",
       "      <td>41990600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>214.520004</td>\n",
       "      <td>219.500000</td>\n",
       "      <td>212.320007</td>\n",
       "      <td>214.449997</td>\n",
       "      <td>214.449997</td>\n",
       "      <td>53124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>220.419998</td>\n",
       "      <td>222.880005</td>\n",
       "      <td>216.839996</td>\n",
       "      <td>222.110001</td>\n",
       "      <td>222.110001</td>\n",
       "      <td>40337900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>221.160004</td>\n",
       "      <td>221.830002</td>\n",
       "      <td>217.270004</td>\n",
       "      <td>217.360001</td>\n",
       "      <td>217.360001</td>\n",
       "      <td>30791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>2018-10-16</td>\n",
       "      <td>218.929993</td>\n",
       "      <td>222.990005</td>\n",
       "      <td>216.759995</td>\n",
       "      <td>222.149994</td>\n",
       "      <td>222.149994</td>\n",
       "      <td>29184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>222.300003</td>\n",
       "      <td>222.639999</td>\n",
       "      <td>219.339996</td>\n",
       "      <td>221.190002</td>\n",
       "      <td>221.190002</td>\n",
       "      <td>22885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>2018-10-18</td>\n",
       "      <td>217.860001</td>\n",
       "      <td>219.740005</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>216.020004</td>\n",
       "      <td>216.020004</td>\n",
       "      <td>32581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>218.059998</td>\n",
       "      <td>221.259995</td>\n",
       "      <td>217.429993</td>\n",
       "      <td>219.309998</td>\n",
       "      <td>219.309998</td>\n",
       "      <td>33078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>2018-10-22</td>\n",
       "      <td>219.789993</td>\n",
       "      <td>223.360001</td>\n",
       "      <td>218.940002</td>\n",
       "      <td>220.649994</td>\n",
       "      <td>220.649994</td>\n",
       "      <td>28760700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "2197  2018-09-24  216.820007  221.259995  216.630005  220.789993  220.789993   \n",
       "2198  2018-09-25  219.750000  222.820007  219.699997  222.190002  222.190002   \n",
       "2199  2018-09-26  221.000000  223.750000  219.759995  220.419998  220.419998   \n",
       "2200  2018-09-27  223.820007  226.440002  223.539993  224.949997  224.949997   \n",
       "2201  2018-09-28  224.789993  225.839996  224.020004  225.740005  225.740005   \n",
       "2202  2018-10-01  227.949997  229.419998  226.350006  227.259995  227.259995   \n",
       "2203  2018-10-02  227.250000  230.000000  226.630005  229.279999  229.279999   \n",
       "2204  2018-10-03  230.050003  233.470001  229.779999  232.070007  232.070007   \n",
       "2205  2018-10-04  230.779999  232.350006  226.729996  227.990005  227.990005   \n",
       "2206  2018-10-05  227.960007  228.410004  220.580002  224.289993  224.289993   \n",
       "2207  2018-10-08  222.210007  224.800003  220.199997  223.770004  223.770004   \n",
       "2208  2018-10-09  223.639999  227.270004  222.250000  226.869995  226.869995   \n",
       "2209  2018-10-10  225.460007  226.350006  216.050003  216.360001  216.360001   \n",
       "2210  2018-10-11  214.520004  219.500000  212.320007  214.449997  214.449997   \n",
       "2211  2018-10-12  220.419998  222.880005  216.839996  222.110001  222.110001   \n",
       "2212  2018-10-15  221.160004  221.830002  217.270004  217.360001  217.360001   \n",
       "2213  2018-10-16  218.929993  222.990005  216.759995  222.149994  222.149994   \n",
       "2214  2018-10-17  222.300003  222.639999  219.339996  221.190002  221.190002   \n",
       "2215  2018-10-18  217.860001  219.740005  213.000000  216.020004  216.020004   \n",
       "2216  2018-10-19  218.059998  221.259995  217.429993  219.309998  219.309998   \n",
       "2217  2018-10-22  219.789993  223.360001  218.940002  220.649994  220.649994   \n",
       "\n",
       "        Volume  \n",
       "2197  27693400  \n",
       "2198  24554400  \n",
       "2199  23984700  \n",
       "2200  30181200  \n",
       "2201  22929400  \n",
       "2202  23600800  \n",
       "2203  24788200  \n",
       "2204  28654800  \n",
       "2205  32042000  \n",
       "2206  33580500  \n",
       "2207  29663900  \n",
       "2208  26891000  \n",
       "2209  41990600  \n",
       "2210  53124400  \n",
       "2211  40337900  \n",
       "2212  30791000  \n",
       "2213  29184000  \n",
       "2214  22885400  \n",
       "2215  32581300  \n",
       "2216  33078700  \n",
       "2217  28760700  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"Price/AAPL.csv\"\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "print(df.shape)\n",
    "df.tail(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df, training_length):\n",
    "    adj_close = df['Adj Close'].values\n",
    "    time_steps = adj_close.shape[0]\n",
    "    features = 1\n",
    "    sequences = time_steps - training_length + 1\n",
    "    x = np.zeros((sequences-1, training_length, features))\n",
    "    y = np.zeros((sequences-1))\n",
    "    for seq in range(sequences-1):\n",
    "        x[seq, :, :] = adj_close[seq:seq+training_length].reshape(training_length, features)   \n",
    "        y[seq] = adj_close[seq+training_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = create_data(df, 40)\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,\n",
    "               activation='sigmoid',\n",
    "               recurrent_activation='sigmoid',\n",
    "               return_sequences=True, \n",
    "               input_shape=(40,1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64,\n",
    "              return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64,\n",
    "              return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',\n",
    "             optimizer='adam',\n",
    "             metrics=['mae', 'mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2178/2178 [==============================] - 40s 18ms/step - loss: 6368.1668 - mean_absolute_error: 62.8881 - mean_squared_error: 6368.1668\n",
      "Epoch 2/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 2803.1298 - mean_absolute_error: 42.9923 - mean_squared_error: 2803.1298\n",
      "Epoch 3/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 2811.7523 - mean_absolute_error: 43.6739 - mean_squared_error: 2811.7523\n",
      "Epoch 4/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 2779.9187 - mean_absolute_error: 43.2253 - mean_squared_error: 2779.9187\n",
      "Epoch 5/40\n",
      "2178/2178 [==============================] - 28s 13ms/step - loss: 2822.7787 - mean_absolute_error: 43.6991 - mean_squared_error: 2822.7787\n",
      "Epoch 6/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 2819.3192 - mean_absolute_error: 43.4071 - mean_squared_error: 2819.3192\n",
      "Epoch 7/40\n",
      "2178/2178 [==============================] - 29s 14ms/step - loss: 2804.6665 - mean_absolute_error: 43.1807 - mean_squared_error: 2804.6665\n",
      "Epoch 8/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 2751.6086 - mean_absolute_error: 42.8097 - mean_squared_error: 2751.6086\n",
      "Epoch 9/40\n",
      "2178/2178 [==============================] - 31s 14ms/step - loss: 1424.4806 - mean_absolute_error: 26.3680 - mean_squared_error: 1424.4806\n",
      "Epoch 10/40\n",
      "2178/2178 [==============================] - 28s 13ms/step - loss: 617.3845 - mean_absolute_error: 17.1389 - mean_squared_error: 617.3845\n",
      "Epoch 11/40\n",
      "2178/2178 [==============================] - 28s 13ms/step - loss: 483.0678 - mean_absolute_error: 15.7689 - mean_squared_error: 483.0678\n",
      "Epoch 12/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 438.4816 - mean_absolute_error: 14.9726 - mean_squared_error: 438.4816\n",
      "Epoch 13/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 480.8453 - mean_absolute_error: 15.6728 - mean_squared_error: 480.8453\n",
      "Epoch 14/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 438.5157 - mean_absolute_error: 14.9433 - mean_squared_error: 438.5157\n",
      "Epoch 15/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 404.8481 - mean_absolute_error: 14.3807 - mean_squared_error: 404.8481\n",
      "Epoch 16/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 408.3900 - mean_absolute_error: 14.5676 - mean_squared_error: 408.3900\n",
      "Epoch 17/40\n",
      "2178/2178 [==============================] - 28s 13ms/step - loss: 437.9522 - mean_absolute_error: 15.1440 - mean_squared_error: 437.9522\n",
      "Epoch 18/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 405.8859 - mean_absolute_error: 14.4963 - mean_squared_error: 405.8859\n",
      "Epoch 19/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 404.2963 - mean_absolute_error: 14.6657 - mean_squared_error: 404.2963\n",
      "Epoch 20/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 386.8907 - mean_absolute_error: 13.9289 - mean_squared_error: 386.8907\n",
      "Epoch 21/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 401.4223 - mean_absolute_error: 14.1660 - mean_squared_error: 401.4223\n",
      "Epoch 22/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 382.3461 - mean_absolute_error: 14.1345 - mean_squared_error: 382.3461\n",
      "Epoch 23/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 355.8046 - mean_absolute_error: 13.3231 - mean_squared_error: 355.8046\n",
      "Epoch 24/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 384.8337 - mean_absolute_error: 13.9266 - mean_squared_error: 384.8337\n",
      "Epoch 25/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 386.8220 - mean_absolute_error: 14.0943 - mean_squared_error: 386.8220\n",
      "Epoch 26/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 385.3860 - mean_absolute_error: 13.7031 - mean_squared_error: 385.3860\n",
      "Epoch 27/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 363.3463 - mean_absolute_error: 13.6610 - mean_squared_error: 363.3463\n",
      "Epoch 28/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 373.4828 - mean_absolute_error: 13.7095 - mean_squared_error: 373.4828\n",
      "Epoch 29/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 380.9126 - mean_absolute_error: 13.8896 - mean_squared_error: 380.9126\n",
      "Epoch 30/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 375.4383 - mean_absolute_error: 13.8160 - mean_squared_error: 375.4383\n",
      "Epoch 31/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 349.4897 - mean_absolute_error: 13.4393 - mean_squared_error: 349.4897\n",
      "Epoch 32/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 378.9322 - mean_absolute_error: 13.6227 - mean_squared_error: 378.9322\n",
      "Epoch 33/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 361.0468 - mean_absolute_error: 13.3685 - mean_squared_error: 361.0468\n",
      "Epoch 34/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 372.1351 - mean_absolute_error: 13.4164 - mean_squared_error: 372.1351\n",
      "Epoch 35/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 379.9401 - mean_absolute_error: 13.5562 - mean_squared_error: 379.9401\n",
      "Epoch 36/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 375.3586 - mean_absolute_error: 13.7389 - mean_squared_error: 375.3586\n",
      "Epoch 37/40\n",
      "2178/2178 [==============================] - 28s 13ms/step - loss: 356.7906 - mean_absolute_error: 13.2792 - mean_squared_error: 356.7906\n",
      "Epoch 38/40\n",
      "2178/2178 [==============================] - 29s 13ms/step - loss: 344.4077 - mean_absolute_error: 13.0337 - mean_squared_error: 344.4077\n",
      "Epoch 39/40\n",
      "2178/2178 [==============================] - 27s 13ms/step - loss: 348.0575 - mean_absolute_error: 13.1003 - mean_squared_error: 348.0575\n",
      "Epoch 40/40\n",
      "2178/2178 [==============================] - 30s 14ms/step - loss: 328.3575 - mean_absolute_error: 12.7776 - mean_squared_error: 328.3575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dcfeb5e0f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=40, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 40, 64)            16896     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 40, 64)            33024     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 87,169\n",
      "Trainable params: 87,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
